#ğŸ“˜ Seq2Seq LSTM Machine Translation â€“ English â†’ French Translator

A Deep Learningâ€“based Encoderâ€“Decoder implementation using LSTMs

â­ Project Overview

This project implements a Sequence-to-Sequence (Seq2Seq) neural network using LSTM layers to translate English sentences into French. The model is trained on an Englishâ€“French sentence dataset and follows an encoderâ€“decoder architecture commonly used in machine translation tasks.

ğŸš€ Key Features

Encoderâ€“decoder architecture using LSTM networks

Custom preprocessing pipeline:

tokenization

vocabulary creation

word-to-index mapping

padding & truncation

Trained on 10,000 Englishâ€“French sentence pairs

Evaluates translation quality using:

BLEU Score

Sample qualitative outputs

Fully implemented in TensorFlow / Keras

ğŸ“‚ Dataset

Source: https://www.manythings.org/anki/fra-eng.zip

Contains Englishâ€“French sentence pairs (fra.txt)

Only the last 10,000 pairs are used for faster training

Dataset structure:

English_sentence \t French_sentence

ğŸ§¹ Data Preprocessing Pipeline

Load the dataset

Clean text (lowercase, remove punctuation)

Tokenize English & French separately

Build vocabularies

Convert text â†’ integer sequences

Pad sequences to a fixed length

Train-test split (80/20)

ğŸ§  Model Architecture
Encoder

Embedding Layer

LSTM Layer (units = 128 / 256 / 512 depending on experiment)

Outputs encoder hidden & cell states

Decoder

Embedding Layer

LSTM Layer receiving encoder state

Dense Layer with Softmax activation

âš™ï¸ Training Configuration

Loss: sparse_categorical_crossentropy

Optimizer: Adam

Metrics: Accuracy

Epochs: configurable

Batch size: configurable

ğŸ“Š Evaluation
Quantitative

BLEU Score using nltk.translate.bleu_score

Evaluation on test split (20%)

Qualitative

Sample input English sentence

Model translation output (French)

Comparison with ground-truth translation

ğŸ”¬ Experiments

You vary the number of LSTM units to compare performance:

128 units

256 units

512 units

You also discuss how sequence length affects:

training stability

translation quality

inference difficulty

ğŸ“ Project Structure
Seq2Seq-LSTM-Translation/
â”‚
â”œâ”€â”€ Seq2Seq LSTM.ipynb     # Full implementation notebook
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ data/                  # (Optional) Dataset files
â””â”€â”€ results/               # BLEU scores, sample outputs

ğŸ› ï¸ Technologies Used

Python

NumPy

TensorFlow / Keras

NLTK

Matplotlib

ğŸ“ How to Run

Download the dataset:

https://www.manythings.org/anki/fra-eng.zip


Extract fra.txt to the project folder.

Run the Jupyter Notebook:

jupyter notebook "Seq2Seq LSTM.ipynb"


Train the model and view results.

ğŸ“Œ Future Improvements

Beam Search decoding

Attention mechanism (Luong or Bahdanau)

Transformer-based model

Support for larger datasets

ğŸ‘¨â€ğŸ’» Author

Janvi Kumari
CS564 â€“ Machine Learning | IIT Patna
